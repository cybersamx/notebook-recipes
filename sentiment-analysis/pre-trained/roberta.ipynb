{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Sentiment Analysis Using the RoBERTa Model\n",
    "\n",
    "[RoBERTa](https://ai.meta.com/blog/roberta-an-optimized-method-for-pretraining-self-supervised-nlp-systems/) is an open-source project at Meta that leverages the self-supervised method BERT for NLP.\n",
    " \n",
    "In this notebook, we perform sentiment analysis on 1 tweet (hardcoded) using the roBERTa model called [twitter-roberta-base-sentiment-latest](https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest) that is trained on ~124M tweets from Twitter.\n",
    "\n",
    "## Labels\n",
    "\n",
    "| Index | Label    |\n",
    "|:-----:|----------|\n",
    "|   0   | Negative |\n",
    "|   1   | Neutral  |\n",
    "|   2   | Positive |\n",
    "\n",
    "## Future Work\n",
    "\n",
    "Use the Amazon product review dataset in the data directory to feed to the model.\n"
   ],
   "id": "c2b269085bc54d39"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T07:54:26.498548Z",
     "start_time": "2024-10-07T07:54:26.496765Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "892efb8cb7c384f1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T07:54:27.750641Z",
     "start_time": "2024-10-07T07:54:26.505233Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Install the packages\n",
    "# Note pytorch must be installed in order for transformers to work.\n",
    "\n",
    "!pip install ipywidgets scipy torch transformers > /dev/null 2>&1\n",
    "!export TOKENIZERS_PARALLELISM=false"
   ],
   "id": "ac70ae21524f786a",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T07:54:27.767474Z",
     "start_time": "2024-10-07T07:54:27.764904Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import packages\n",
    "\n",
    "import os\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    logging,\n",
    ")\n",
    "from scipy.special import softmax\n",
    "\n",
    "# Suppress warnings (as it can get noisy sometimes).\n",
    "logging.set_verbosity_error()"
   ],
   "id": "b64c464a3d7850be",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T07:54:27.779898Z",
     "start_time": "2024-10-07T07:54:27.777228Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tweet = \"@cybersam terrible day @ beach ☹️ https://latimes.com\"\n",
    "\n",
    "# A tweet can comprise a user mention (prefix with @) and link. We need to replace those\n",
    "# tokens with special literals @user and http respectively for the model to recognize. \n",
    "\n",
    "def modify(token):\n",
    "    if token.startswith(\"@\") and len(token) > 1:\n",
    "        return '@user'\n",
    "    if token.startswith(\"http\"):\n",
    "        return 'http'\n",
    "    if token.startswith(\"https\"):\n",
    "        return 'https'\n",
    "    \n",
    "    return token\n",
    "\n",
    "clean_tweet = ' '.join([modify(token) for token in tweet.split()])\n",
    "print(clean_tweet)\n",
    "        "
   ],
   "id": "d0f5df6554d8d741",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@user terrible day @ beach ☹️ http\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "> **Notes**\n",
    "> 1. If you are use the model `twitter-roberta-base-sentiment-latest` for the first time, the operation will take 1-5 minutes for the model will be downloaded from hugging face.\n",
    "> 1. If you see this error: `ImportError: AutoModelForSequenceClassification requires the PyTorch library but it was not found in your environment`, please restart your Notebook kernel and rerun the entire notebook so that the kernel and reload any packages that aren't explicitly imported by this notebook.\n"
   ],
   "id": "cc1c0e1cd457067d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T07:54:28.726316Z",
     "start_time": "2024-10-07T07:54:27.788500Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Run the model and tokenizer\n",
    "\n",
    "# See model home page on hugging face:\n",
    "# https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest\n",
    "\n",
    "# Install the model name from hugging face.\n",
    "model_name = 'cardiffnlp/twitter-roberta-base-sentiment-latest'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, clean_up_tokenization_spaces=True)\n",
    "\n",
    "# For labels, get the label definitions from the hugging face.\n",
    "labels = ['Negative', 'Neutral', 'Positive']"
   ],
   "id": "37759e5a77dafe6c",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T07:54:28.735671Z",
     "start_time": "2024-10-07T07:54:28.733642Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Tokenize/vectorize the text.\n",
    "\n",
    "# Encode the tweet as pytorch (pt) tensors\n",
    "tokens = tokenizer.encode(tweet, return_tensors='pt')\n",
    "print(tokens)"
   ],
   "id": "649becfbef661e9d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,  1039,  4469,  9569,   424,  6587,   183,   787,  4105, 42699,\n",
      "          9253, 12605,  1205,   640, 12805,  9452,     4,   175,     2]])\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T07:54:28.800201Z",
     "start_time": "2024-10-07T07:54:28.742273Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Feed the tokenized tweet to the model.\n",
    "\n",
    "result = model(tokens)\n",
    "print(result)"
   ],
   "id": "84913b31a4614290",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.4574, -0.2846, -2.4405]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T07:54:28.810032Z",
     "start_time": "2024-10-07T07:54:28.807627Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Make the output more friendly and relevant.\n",
    "\n",
    "raw_scores = result.logits[0].detach().numpy()\n",
    "print(f'Raw scores: {raw_scores}')\n",
    "\n",
    "# Let's convert the raw scores to probabilities mapping to the labels.\n",
    "probabilities = softmax(raw_scores)\n",
    "for k, v in zip(labels, probabilities):\n",
    "    print(f'{k}: {v:.3f}')"
   ],
   "id": "17cc2f3c51f97893",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw scores: [ 2.4574018  -0.28458977 -2.4404929 ]\n",
      "Negative: 0.933\n",
      "Neutral: 0.060\n",
      "Positive: 0.007\n"
     ]
    }
   ],
   "execution_count": 31
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
